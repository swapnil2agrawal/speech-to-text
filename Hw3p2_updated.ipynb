{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('homework-3-part-2-11-785-fall-2019.zip', 'r') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import *\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HW3P2_Data import phoneme_list as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme = data.PHONEME_LIST\n",
    "phoneme_map = data.PHONEME_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_map.insert(0, ' ')\n",
    "phoneme.insert(0, 'BLANK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalid = np.load('./HW3P2_Data/wsj0_dev.npy', encoding=\"latin1\")\n",
    "yvalid = np.load('./HW3P2_Data/wsj0_dev_merged_labels.npy')\n",
    "xtrain = np.load('./HW3P2_Data/wsj0_train.npy', encoding=\"latin1\")\n",
    "ytrain = np.load(r'./HW3P2_Data/wsj0_train_merged_labels.npy')\n",
    "xtest = np.load('./HW3P2_Data/wsj0_test.npy', encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 40)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and dataset\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "#         if y is not None:\n",
    "#             self.total_phonemes = sum(len(yi) for yi in y)\n",
    "#         else:\n",
    "#             self.total_phonemes = -1\n",
    "\n",
    "#         print(\"n_utters\", self.x.shape[0], \"total_phonemes\", self.total_phonemes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.x[idx]\n",
    "        return torch.from_numpy(frames).float(), \\\n",
    "               torch.from_numpy(self.y[idx] + 1 if self.y is not None else np.array([-1])).int()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "#     print(\"----\", batch[0])\n",
    "    batch = sorted(batch, key=lambda b: b[0].size(0), reverse=True)  # sort the batch by seq_len desc\n",
    "#     print(batch[0])\n",
    "#     key=lambda b: b[0].size(0)\n",
    "#     print(\"....\", key)\n",
    "    \n",
    "    max_seq_len = batch[0][0].size(0)\n",
    "    channels = batch[0][0].size(1)\n",
    "    pack = torch.zeros(max_seq_len, batch_size, channels)\n",
    "    all_labels = []\n",
    "    seq_sizes = []\n",
    "    label_sizes = torch.zeros(batch_size).int()\n",
    "    \n",
    "    for i, (frames, labels) in enumerate(batch):\n",
    "        seq_size = frames.size(0)\n",
    "        seq_sizes.append(seq_size)\n",
    "\n",
    "        labele_size = labels.size(0)\n",
    "        label_sizes[i] = labele_size\n",
    "\n",
    "        pack[:seq_size, i, :] = frames\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return pack, seq_sizes, all_labels, label_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        MyDataset(xtrain, ytrain),\n",
    "        batch_size=100, shuffle=False, collate_fn=my_collate)\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(xvalid, yvalid),\n",
    "    batch_size=100, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "#         self.embed = nn.Embedding(vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.output = nn.Linear(hidden_size * 2, vocab)\n",
    "    \n",
    "    def forward(self, X, lengths):\n",
    "#         X = self.embed(X)\n",
    "        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
    "        packed_out = self.lstm(packed_X)[0]\n",
    "        out, out_lens = pad_packed_sequence(packed_out)\n",
    "        # Log softmax after output layer is required for use in `nn.CTCLoss`.\n",
    "        out = self.output(out).log_softmax(2)\n",
    "        return out, out_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lstm): LSTM(40, 4, bidirectional=True)\n",
       "  (output): Linear(in_features=8, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.manual_seed(11785)\n",
    "model = Model(len(phoneme), 40, 4)\n",
    "criterion = nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader):\n",
    "    \n",
    "    model.train()\n",
    "    numEpochs = 1\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        \n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch, (frames, seq_sizes, labels, label_sizes) in enumerate(data_loader):\n",
    "            \n",
    "            frames = frames.to(device)  #, \n",
    "            seq_sizes = torch.IntTensor(seq_sizes)\n",
    "            seq_sizes = seq_sizes.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            print(seq_sizes)\n",
    "            output, out_lens = model(frames, seq_sizes)\n",
    "            print(output.shape)\n",
    "            print(out_lens)\n",
    "            \n",
    "            print(\"----\")\n",
    "            print(label_sizes)\n",
    "            print(out_lens)\n",
    "            # output - probability of every phoneme for every frame \n",
    "            labels = torch.cat(labels).int().to(device)\n",
    "            loss = criterion(output, labels, out_lens, label_sizes)\n",
    "            loss.backward()\n",
    "            optimizer.step()          \n",
    "            avg_loss += loss.item()\n",
    "       \n",
    "            if batch % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch+1, avg_loss/50))\n",
    "                avg_loss = 0.0\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            del labels\n",
    "            del loss\n",
    "            break\n",
    "        print(\"time: \", time.time()-start)\n",
    "        PATH = \"./\" + str(epoch) + \".pt\"\n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1237, 1217, 1057, 1043, 1014,  997,  947,  944,  914,  896,  889,  886,\n",
      "         850,  834,  832,  829,  822,  818,  810,  784,  762,  722,  700,  697,\n",
      "         694,  693,  690,  668,  642,  623,  611,  609,  609,  609,  581,  576,\n",
      "         568,  537,  528,  521,  519,  518,  511,  509,  506,  504,  502,  499,\n",
      "         497,  496,  495,  494,  492,  482,  481,  480,  479,  479,  478,  476,\n",
      "         468,  466,  464,  464,  463,  462,  461,  458,  458,  457,  452,  446,\n",
      "         446,  445,  445,  445,  444,  441,  440,  440,  436,  432,  427,  426,\n",
      "         418,  418,  416,  408,  405,  403,  399,  397,  394,  332,  326,  324,\n",
      "         281,  194,  193,  151], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1237, 100, 47])\n",
      "tensor([1237, 1217, 1057, 1043, 1014,  997,  947,  944,  914,  896,  889,  886,\n",
      "         850,  834,  832,  829,  822,  818,  810,  784,  762,  722,  700,  697,\n",
      "         694,  693,  690,  668,  642,  623,  611,  609,  609,  609,  581,  576,\n",
      "         568,  537,  528,  521,  519,  518,  511,  509,  506,  504,  502,  499,\n",
      "         497,  496,  495,  494,  492,  482,  481,  480,  479,  479,  478,  476,\n",
      "         468,  466,  464,  464,  463,  462,  461,  458,  458,  457,  452,  446,\n",
      "         446,  445,  445,  445,  444,  441,  440,  440,  436,  432,  427,  426,\n",
      "         418,  418,  416,  408,  405,  403,  399,  397,  394,  332,  326,  324,\n",
      "         281,  194,  193,  151])\n",
      "----\n",
      "tensor([142, 147, 129, 129, 122, 121, 118, 112,  96, 107, 110, 120, 104, 107,\n",
      "        107, 117, 107, 112,  99, 102,  93, 101,  95,  86,  94,  96,  87,  86,\n",
      "         74,  72,  75,  80,  80,  78,  60,  64,  79,  58,  61,  69,  60,  63,\n",
      "         68,  60,  65,  66,  62,  62,  62,  66,  60,  60,  50,  64,  58,  60,\n",
      "         60,  66,  60,  61,  62,  52,  60,  64,  57,  58,  61,  61,  63,  60,\n",
      "         64,  58,  53,  63,  61,  63,  52,  58,  54,  60,  58,  54,  54,  51,\n",
      "         54,  57,  64,  56,  47,  48,  55,  54,  56,  29,  46,  27,  34,  20,\n",
      "         19,  14], dtype=torch.int32)\n",
      "tensor([1237, 1217, 1057, 1043, 1014,  997,  947,  944,  914,  896,  889,  886,\n",
      "         850,  834,  832,  829,  822,  818,  810,  784,  762,  722,  700,  697,\n",
      "         694,  693,  690,  668,  642,  623,  611,  609,  609,  609,  581,  576,\n",
      "         568,  537,  528,  521,  519,  518,  511,  509,  506,  504,  502,  499,\n",
      "         497,  496,  495,  494,  492,  482,  481,  480,  479,  479,  478,  476,\n",
      "         468,  466,  464,  464,  463,  462,  461,  458,  458,  457,  452,  446,\n",
      "         446,  445,  445,  445,  444,  441,  440,  440,  436,  432,  427,  426,\n",
      "         418,  418,  416,  408,  405,  403,  399,  397,  394,  332,  326,  324,\n",
      "         281,  194,  193,  151])\n",
      "time:  0.1435565948486328\n"
     ]
    }
   ],
   "source": [
    "train(model, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ctcdecode import CTCBeamDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?CTCBeamDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(output, seq_sizes, beam_width=40, PHONEME_MAP = phoneme_map, phoneme = phoneme):\n",
    "    \n",
    "    decoder = CTCBeamDecoder(labels = PHONEME_MAP, blank_id=0, beam_width=beam_width)\n",
    "    output = torch.transpose(output, 0, 1)  # batch, seq_len, probs\n",
    "    output1, _, _, out_seq_len = decoder.decode(probs = output,\n",
    "                                                            seq_lens= seq_sizes)\n",
    "#     print(\"output\", output1)  #N, B, T\n",
    "    print(output1.shape)\n",
    "#     print(\"out_seq_len\", out_seq_len) # N, B\n",
    "    print(out_seq_len.shape)\n",
    "    \n",
    "    decoded = []\n",
    "    Sentence = []\n",
    "    for i in range(output1.size(0)):\n",
    "        chrs = \"\"\n",
    "        sen = \"\"\n",
    "        if out_seq_len[i, 0] != 0:\n",
    "#             print(output1[i, 0, :out_seq_len[i, 0]])\n",
    "            chrs = \"\".join(PHONEME_MAP[o] for o in output1[i, 0, :out_seq_len[i, 0]])\n",
    "            sen = \"\".join(phoneme[o] for o in output1[i, 0, :out_seq_len[i, 0]])\n",
    "        decoded.append(chrs)\n",
    "        Sentence.append(sen)\n",
    "        \n",
    "    return decoded, Sentence\n",
    "\n",
    "\n",
    "def predict(loader):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_cer = 0\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for batch, (frames, seq_sizes, labels, label_sizes) in enumerate(loader):\n",
    "\n",
    "        frames = frames.to(device)  #, \n",
    "        seq_sizes = torch.IntTensor(seq_sizes)\n",
    "        seq_sizes = seq_sizes.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        output, out_lens = model(frames, seq_sizes)\n",
    "\n",
    "        labels = torch.cat(labels).int().to(device)\n",
    "        loss = criterion(output, labels, out_lens, label_sizes)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        del labels\n",
    "        del loss\n",
    "\n",
    "        decoded, Sen = decode(output, seq_sizes)\n",
    "#         break\n",
    "        \n",
    "    return decoded, Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 40, 1237])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1335])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1179])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1579])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1743])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1629])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1439])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1237])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1567])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1443])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([100, 40, 1153])\n",
      "torch.Size([100, 40])\n",
      "torch.Size([6, 40, 1080])\n",
      "torch.Size([6, 40])\n"
     ]
    }
   ],
   "source": [
    "decoded, Sen = predict(dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sen[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"\"\n",
    "B = \"\"\n",
    "for i in range(len(decoded[5])):\n",
    "    A += decoded[5][i]\n",
    "    B += Sen[5][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = None\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(xtest, ytest),\n",
    "    batch_size= 1, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[943]\n",
      "[tensor([-1], dtype=torch.int32)]\n",
      "tensor([1], dtype=torch.int32)\n",
      "torch.Size([943, 1, 40])\n"
     ]
    }
   ],
   "source": [
    "for batch, (frames, seq_sizes, labels, label_sizes) in enumerate(test_loader):\n",
    "    print(batch)\n",
    "    print(seq_sizes)\n",
    "    print(labels)\n",
    "    print(label_sizes)\n",
    "    print(frames.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(loader):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_cer = 0\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for batch, (frames, seq_sizes, _, _) in enumerate(loader):\n",
    "\n",
    "        frames = frames.to(device)  #, \n",
    "        seq_sizes = torch.IntTensor(seq_sizes)\n",
    "        seq_sizes = seq_sizes.to(device)\n",
    "\n",
    "        output, out_lens = model(frames, seq_sizes)\n",
    "        decoded, Sen = decode(output, seq_sizes)\n",
    "        break\n",
    "    return decoded, Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 943])\n",
      "torch.Size([1, 40])\n"
     ]
    }
   ],
   "source": [
    "decoded = predict_test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pv@uYwmsD~oT~Gk@+HDHj.uUO~t-ek.@OimAuug_SuwYO@.gf+uEu-is+DafaUIek~gk~ayj.o@T?Zjl~kSAtSlTZkmeIwyrclb!ZfSuudhTZbyb!-?ls_RHkYSdaelses@@@hvhwhhSvwffZ@EwfIIhfY@EmzsZ!G?E????!?mY@iydt-hYHHUYdfeS-Hh@k.SYp@.eY+jGjfiZEufZasj.wSUpTkt@uhvdel.YUZ+hhhyh-IIovSyHSA-U-TSafDs.awwZZehSuffYSSk@ts@ijSGIS__mvufwUYwSgjejkwsuHHZhWa_wYS@@lm_Tv@pEo.~ffbIb.bgDD@IaelffluwewwfEo@@-uwIhDdcHygkaHYEEDDHYs~IshwUlGpGhSuudU-??o~ptpHHkjHU?HY?lSS@SlHHuSeva.W~.pflpWzfEEwuTfZfZveh-fkH@E-!fW?R.p_gyZ!fspWffSuujzY?wfE?Yf@Howg!TfSiuul@kSbDh.UoGjvvD?ln-++SyygoiDpgpjOjpppnIDsl-?WakpvkuU?AAoDA-!!IuuDbIf@@@Ii@gtIj@a-AU-aka@SASateukDyy?Wec~pO~Dyaudp@j@@@@@@@offffffffffEwZrDT-vhffffvIA?gTl!lAjkZdITpkdghjbekm!yiGGafweevv-tcIEh--fu-TsYa-AzUDGuhppppg@oHHSokIapjShaIi@DzEUDTkpgpjynTsAAA?iiaAaHgOgogjgegopApIs@?-St@?@ujTjuG!el@SUumWm!f_i+_!_.gYkwZiuYSYIi+?aw??+iE_vvSvu?@WIIYmff@@_IYbk@f@Y!O!f@oyveu+pl?uYHj.ST@aeSYkUkk.kSOtt.TT?oOS-uHgggImI~mI.@fji~wlTaAia~eaZwed@lsZswSjjjjj@SvjD_gjIW'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(decoded[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
